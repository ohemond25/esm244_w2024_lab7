---
title: "ESM 244 Week 7: Binary logistic regression and `tidymodels`"
author: "Olivia Hemond"
date: "2024-02-22"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
execute:
  warning: false
  message: false
---

```{r setup}
### Packages you may need to install: tidymodels, ranger (for random forest)
library(tidyverse) 
library(here)

### a metapackage like tidyverse, tidymodels contains many model-relevant 
### packages incl rsample, parsnip, recipes, yardstick, broom - we don't 
### need to worry about the differences here...
library(tidymodels) 
```

# Intro

For this workshop/tutorial, we will use the `tidymodels` package to quickly generate and test predictive models, here focusing on classification models (predicting a categorical output based on various inputs).

This fits squarely within the Tidyverse data science cycle:

![](../img/data_cycle.png)

This borrows a lot from the Posit `tidymodels` tutorial that can be found here - visit these pages for more in-depth exploration of the `tidymodels` package:

* [https://www.tidymodels.org/start/models/](https://www.tidymodels.org/start/models/)    
* [https://www.tidymodels.org/start/recipes/](https://www.tidymodels.org/start/recipes/)
* [https://www.tidymodels.org/start/resampling/](https://www.tidymodels.org/start/resampling/)
* [https://www.tidymodels.org/start/tuning/](https://www.tidymodels.org/start/tuning/)
* [https://www.tidymodels.org/start/case-study/](https://www.tidymodels.org/start/case-study/)


# Tidymodels with a classifier task

Classification task: distinguish between different categorical variables. Can have binary outcomes (our focus here), or multinomial outcomes, or ordinal outcomes. 

Let's use the titanic dataset (the complex one from the `titanic` R package, not the simplified version available in base R).  This dataset contains information on passengers including name, age, ticket number, passenger class, sex... and whether they survived the tragedy or not.  Here we can create a model to predict survival (survived/didn't survive) based on various predictor variables.  Because we're predicting a categorical outcome (survived/didn't), this is a "classification" task.

Consider: what is an example of a categorical outcome you might see in your own research/studies, that would be relevant to a classification task?

* Species threatened status (threatened vs. not threatened)
* Species identity (coastal live oak vs. interior live oak)
* Support for a ballot proposition (likely to vote for vs. against).

## Examining the data

```{r writing out data from titanic package}
#| eval: false
#| echo: false
### Used titanic package in R to get our data
t_df <- titanic::titanic_train %>%
  janitor::clean_names()
write_csv(t_df, here('data/titanic/titanic_survival.csv'))
```

```{r load titanic data}
### Get data from folder
t_df <- read_csv(here('data/titanic/titanic_survival.csv'))
### View(t_df) ### examine the data
### summary(t_df)

### Let's do a little processing to create a "survival" dataframe
# Remember: 1 = survived, 0 = did not survive
surv_df <- t_df %>%
  mutate(survived = factor(survived),   ### "survived" needs to be categorical (factors) rather than just 0/1 (numeric)
         pclass   = factor(pclass)) %>% ### turn the passenger's class into a factor
  select(-cabin, -ticket, -parch, -sib_sp) ### remove because lots of NAs - and not likely to be very helpful

### Exploratory plots: try sex, fare, etc
# Looking to figure out which variables might help us predict survival
ggplot(surv_df, aes(x = pclass, fill = survived)) +
  geom_bar() 
# This looks useful, very few 3rd class survived

ggplot(surv_df, aes(x = age, fill = survived)) +
  geom_histogram()
# very young children survived more. could be helpful to include

ggplot(surv_df, aes(x = sex, fill = survived)) +
  geom_bar()
# more women than men survived

ggplot(surv_df, aes(x = survived, y = fare)) +
  geom_boxplot()
# maybe those that paid higher fares survived more
```

Which predictor variables seem like they might be good at predicting survival (TRUE vs FALSE)?

# Basic BLR in R

(BLR = binary logistic regression)

The `glm` function in base R provides access to generalized linear models.  There are other packages and functions that do as well, but let's run with this one first.

Let's set up two competing models so we can compare using model selection methods.  One will be based on passenger sex and class and, which logically, and from our exploration, seem like good predictors of survival vs. not survival.  The other model will be an essentially random model, based on passenger ID and the port they embarked from, which logically wouldn't seem to have much relationship to survival.

```{r}
### Create two formulas to compare 
f1 <- survived ~ sex + pclass + fare # these all seemed to be solid predictors in our exploration
f2 <- survived ~ passenger_id + embarked # likely not a great model, doubtful these are good predictors

### Run first model using our selected dataset
blr1 <- glm(formula = f1, data = surv_df, family = binomial) # remember to tell it the type of regression (binomial)
summary(blr1)
# males less likely to survive than females
# class 2 and 3 passengers less likely to survive than class 1 passengers
# fare not a significant predictor

### Run second model
blr2 <- glm(formula = f2, data = surv_df, family = binomial)
summary(blr2)
# there actually does seem to be some significant to embarcation location (could be colinear w passenger class)

table(surv_df %>% select(embarked, pclass))
# pclass does vary with embarcation! in Queenstown, vast majority of passengers were third class
```

## Compare model coefficients

Examine and explain the coefficients for each model.

Note: embarkation codes are:

* S = Southampton, England
* C = Cherbourg, France
* Q = Queenstown (Cobh), Ireland

```{r why does embarkation matter}
table(t_df %>% select(embarked, pclass))
```


### Pseudocode to compare the two competing models

decide some k-fold cross validation
take subsection of our data for training, the rest for testing
train model with subsection of data
test model on the rest of the data
iterate (maybe use purrr) (maybe make a function)
AIC or BIC to compare models
running RMSE (jk - won't be useful here, better for scoring continuous outputs not categorical)
use accuracy or area under the curve to pick better model

- both AIC/BIC and k-fold cross validation are ways to balance goodness of model fit with simplicity of the model
- can be preferable to use cross validation for this

### For-loops and purrr::map

We could do a cross validation using for-loops or purrr::map() as in the previous two labs, but let's try some more high-end modeling capability designed to work well with the `tidyverse` ecosystem: `tidymodels`!

# Using `tidymodels`

## Split the data

We will set aside ("partition") a portion of the data for building and comparing our models (80%), and a portion for testing our models after we've selected the best one (20%).  NOT quite the same as folds - that will happen in the training/validation step.

```{r split the data}
### Check balance of survived column (is either outcome super rare? or are they both well represented)
surv_df %>%
  group_by(survived) %>%
  summarize(n = n()) %>% # count number of survivals vs non-survivals
  ungroup() %>%
  mutate(prop = n / sum(n)) # change count into the proportions for survivals / no survivals
### if very unbalanced, choose a stratified split to make sure there are enough
### survivors in the test and training splits.
# result: about 60% didn't survive, about 40% did. pretty close, but we will use a stratified split for funsies

set.seed(123) # keep consistent for reproducibility

surv_split <- initial_split(surv_df, prop = 0.80, strata = survived) # taking 80% for training data (aka 5 fold cross validation)
  ### stratified on `survived`; training and test splits will both have ~60/40% survived = 0/1
  ### will do the cross validation on the 80% portion and select best model. willl leave the other 20% of the data for final verification

# tidymodels helps us set this up easily!
surv_train_df <- training(surv_split) 
surv_test_df <- testing(surv_split)
```

## `tidymodels`: Basic model with `parsnip`

We can set up a basic logistic regression model using functions from the `parsnip` package, which contains a bunch of different model types, and links to multiple model engines for each type (e.g., there are multiple R packages that can calculate a linear model).  The `parsnip` package consolidates a lot of these and helps make the parameters, arguments, and results consistent.

We'll use a binary logistic regression, which predicts the probability (technically, log odds) of outcome A vs. outcome B (e.g., survived vs. did not survive) based on a linear combination of predictors (e.g., passenger class, sex, fare).

```{r set up a binary logistic regression model with our data}
blr_mdl <- logistic_reg() %>% # this function says that our model will start w a logistic regression for binary outcomes
  set_engine('glm') ### this is the default - we could try engines from other packages or functions
    # glm = generalized linear models

# fit the model to our data: feed logistic regression into the formula we had before
blr1_fit <- blr_mdl %>%
  fit(formula = f1, data = surv_train_df)

# let's also create a model we know will be bad:
garbage_fit <- blr_mdl %>%
  fit(formula = f2, data = surv_train_df)

# examine
# see coefficient values (this time aligned horizontally). no p values shown
blr1_fit 
garbage_fit

# how to interpret coefficients: raise e to the coefficient value (times variable value), that will tell you the magnitude of the effect (difference in probability)
```

### Examine the coefficients

Why do the coefficients not match those in `blr1`?

Males are far less likely to survive than reference class female (negative means lower odds which translates to lower probability); passenger classes 2 and 3 are also less likely to survive than reference class 1.

How well does this model predict survival of the test dataset?  Let's use our fitted model from the training set on the test set, and create a confusion matrix to see how well the predictions line up.

### Predict our testing data

```{r}
surv_test_predict <- surv_test_df %>%
  ### straight up prediction, based on 50% prob threshold (to .pred_class):
    # predict surival outcome based upon blr1_fit model using our new set of test data
    # assumes 50% cutoff for making a decision based on probability
  mutate(predict(blr1_fit, new_data = surv_test_df)) %>% 
  ### but can also get the raw probabilities of class A vs B (.pred_A, .pred_B):
    # note: R recognizes . to be surv_test_df as a shortcut for the current dataframe
    # types = "prob" gives the probabilities of each outcome (can then see if you might want to switch your threshold cutoff)
    # probability columns always add up to 1
  mutate(predict(blr1_fit, new_data = ., type = 'prob'))
```

Examine the relationship between `.pred_class`, `.pred_A`, and `.pred_B`.  Where do you suppose the cutoff is to determine `.pred_class`?

```{r}
# Let's compare the actual survived column to the predicted survived column
table(surv_test_predict %>%
        select(survived, .pred_class))
# how many were predicted wrong? (either survived when they didn't, or vice versa)
# confusion matrix

#         .pred_class
# survived  0  1
#        0 91 19
#        1 17 52
```

Try it with a new formula above, can you find a model that improves accuracy?  How much better or worse is a model based only on sex or only on passenger class?

Metrics: we can use metrics from the `yardstick` package (within `tidymodels`) to test accuracy and the Receiver Operating Characteristic curve...

```{r}
# Let's look at accuracy now
# must tell it which column is the real observations, and which is the estimated outcomes
accuracy(surv_test_predict, truth = survived, estimate = .pred_class)
  # result: accuracy is about 80%
```

![from https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/](../img/roc-curve-v2.png)
```{r}
# What if we want to create a dataframe to compare accuracy between versions of our model that have different threshold values?
  # Must again tell it what the real observations are
  # Now we use the probability of predicting 0
roc_df <- roc_curve(surv_test_predict, truth = survived, .pred_0)
autoplot(roc_df)
  # examine plot to see how good our model is at classifying (false positive rate vs true positive rate)

### how about our garbage model? do the same steps as above
garbage_test_df <- surv_test_df %>%
  mutate(predict(garbage_fit, new_data = .)) %>% 
  mutate(predict(garbage_fit, new_data = ., type = 'prob')) 

accuracy(garbage_test_df, truth = survived, estimate = .pred_class)
  # accuracy is about 67%, honestly not that bad

garbage_roc_df <- garbage_test_df %>%
  roc_curve(truth = survived, .pred_0) 

autoplot(garbage_roc_df)
  # result: very close to the random classifier line, pretty clear this is a bad predictor

### Calculate area under curve - 50% is random guessing, 100% is perfect classifier
yardstick::roc_auc(surv_test_predict, truth = survived, .pred_0) # about 85% (very good classifier)
yardstick::roc_auc(garbage_test_df, truth = survived, .pred_0) # about 50% (pretty much equivalent to random guessing)
  # if there's a lot of area under your curve, model is pretty good, regardless of threshold value
  # only works with binary classification scheme
```

### So what?

We basically could have done all that the old way... why would I want to use `tidymodels`?

* `parsnip` standardizes different models and engines from across a wide range of packages and algorithms - we can easily change the binary logistic regression engine to a different package without having to change anything else in our code, or even an entirely different model (e.g., random forest) with minimal changes.
* `tidymodels` also includes other features for more advanced model creation and cross validation.

## `tidymodels`: Cross validation!

We can take our `surv_train_df` and split it out into folds using functions from `rsample` package (another part of `tidymodels`):

```{r}
# we will use cross validation on just the train dataset (80% of our data) so we can then compare final results with full dataset
set.seed(10101)
surv_train_folds <- vfold_cv(surv_train_df, v = 10) # v-fold cross-validation. lets you set number of partitions (here it's 10)
surv_train_folds # stored each fold as list
```

Automates that first step we did!

Now let's create a `workflow` that combines our model and a formula.  We already specified a binary logistic regression model above.  The workflow specifies how R will operate across all the folds.
```{r}
### Remember: this was the basic setup we had before
# blr_mdl <- logistic_reg() %>%
#   set_engine('glm') ### this is the default - we could try engines from other packages or functions

### Create a workflow object
blr_wf <- workflow() %>% # initialize workflow
  add_model(blr_mdl) %>% # adding our model that we created before
  add_formula(survived ~ pclass + sex) # specifying the formula for our model. could use f1 or f2 from above, but we went for a third option here
  # could easily change formula to compare with another option
```

OK now let's apply the workflow to our folded training dataset, and see how it performs!

```{r}
# Let's give it our dataframe, and our workflow, and have it apply our model to each fold then give us the results
blr_fit_folds <- blr_wf %>%
  fit_resamples(surv_train_folds) # computes across multiple resamples

blr_fit_folds
  # .metrics tells us how well everything performed

### Average the predictive performance of the ten models:
collect_metrics(blr_fit_folds)
  # givues us the accuracy of 78% and an roc area under the curve of 0.83
```

With this workflow setup, we can change the formula and rerun the entire process easily to compare different variations on our model. Could tally up metrics for each formula, compare, and select the best one.

Once we know which model we want, we could train it using our entire dataset to get final output

### let's switch up the model, let's try random forest! (completely different type of model)

```{r}
rf_mdl <- rand_forest(trees = 1000) %>%
  set_engine('ranger') %>% ### this is the default - other engines available
  set_mode('classification') ### RF can do classification OR regression; need to specify!

rf_wf <- workflow() %>%   ### initialize workflow
  add_model(rf_mdl) %>%
  add_formula(survived ~ pclass + sex)
  # add_formula(survived ~ pclass + sex + fare)
```

OK now let's apply the workflow to our folded training dataset, and see how it performs!

```{r}
rf_fit_folds <- rf_wf %>%
  fit_resamples(surv_train_folds)

rf_fit_folds

### Average the predictive performance of the ten models:
collect_metrics(rf_fit_folds)
```

## Last fit!

We tried a logistic regression and random forest model on our training data, using cross validation to resample the training data and see how each model performed.  With the fare included as a predictor, the random forest modestly outperformed the logistic regression in the crossvalidation step.

```{r}
last_rf_fit <- rf_wf %>%
  last_fit(surv_split)
collect_metrics(last_rf_fit)
```


# `tidymodels`: Other fancy stuff

Other things you could explore with tidymodels: 

* `recipes` to pre-process your data especially for more complex datasets
* Tuning model hyperparameters - e.g., for random forest, how many decision trees, how many predictors per tree, tree max depth; for neural networks, how many hidden units, etc.

